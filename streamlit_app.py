# streamlit_app.py
# -*- coding: utf-8 -*-
"""
ì„œìš¸ì‹œ íêµ & ì¸êµ¬ì†Œë©¸ ëŒ€ì‹œë³´ë“œ (Streamlit + GitHub Codespaces)

âœ… ë°ì´í„° ì¶œì²˜ ìš°ì„ ìˆœìœ„: KOSTAT, MOIS, SGIS, KEIS, data.go.kr (ê°€ëŠ¥í•˜ë©´ API, ë¶ˆê°€ì‹œ ê³µì‹ CSV/JSON)
âœ… ì‹¤íŒ¨ ì‹œ: ì¬ì‹œë„ â†’ ëŒ€ì²´ ì¶œì²˜ íƒìƒ‰ â†’ ìµœì¢… ì˜ˆì‹œë°ì´í„° ìë™ ìƒì„± (í™”ë©´ ë°°ë„ˆ ì•ˆë‚´)
âœ… ì§€ë„: ì„œìš¸ì‹œ ìì¹˜êµ¬ ê²½ê³„ Choropleth (SGIS ìš°ì„ ) + íêµ ì (ì¢Œí‘œ ì—†ìœ¼ë©´ êµ¬ ì¤‘ì‹¬)
âœ… í‘œì¤€ ìŠ¤í‚¤ë§ˆ: date, value, group(optional)

ì°¸ê³ ìš© ì£¼ì„(ì¸ì¦ ì•ˆë‚´, ë¬¸ì„œ ë§í¬)
- KOSIS OpenAPI: https://kosis.kr/openapi/index/index.jsp  (í™˜ê²½ë³€ìˆ˜: KOSIS_API_KEY)
- data.go.kr ì¼ë°˜í‚¤: https://www.data.go.kr  (í™˜ê²½ë³€ìˆ˜: DATA_GO_KR_KEY)
- SGIS(í†µê³„ì§€ë¦¬ì •ë³´ì„œë¹„ìŠ¤) API: https://sgis.kostat.go.kr  (í™˜ê²½ë³€ìˆ˜: SGIS_ACCESS_KEY, SGIS_SECRET_KEY)
- KEIS(í•œêµ­êµìœ¡í•™ìˆ ì •ë³´ì›) / êµìœ¡ë¶€ íêµ ìë£Œ(ì˜ˆ: data.go.krì˜ íêµ í˜„í™© ë°ì´í„°ì…‹ ê²€ìƒ‰)

Kaggle ì‚¬ìš© ì‹œ (ì„ íƒ): 
- í† í° íŒŒì¼ ~/.kaggle/kaggle.json or í™˜ê²½ë³€ìˆ˜ KAGGLE_USERNAME, KAGGLE_KEY
- ì˜ˆ: !pip install kaggle && kaggle datasets download <dataset> -p ./data
"""

from __future__ import annotations
import os
import io
import json
import math
from datetime import datetime, date, timedelta, timezone
from dateutil.tz import gettz
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
import requests
import streamlit as st
import plotly.express as px
import pydeck as pdk

# ---------------------------
# í™˜ê²½/ìƒìˆ˜
# ---------------------------
APP_TITLE = "ì„œìš¸ì‹œ íêµ & ì¸êµ¬ì†Œë©¸ ëŒ€ì‹œë³´ë“œ"
APP_DESC = "ê³µì‹ ê³µê°œ ë°ì´í„°ë¥¼ ìš°ì„ ì ìœ¼ë¡œ í™œìš©í•˜ì—¬ ì„œìš¸ì‹œ ìì¹˜êµ¬ ë‹¨ìœ„ì˜ íêµ í˜„í™©ê³¼ ì¸êµ¬ì†Œë©¸ ì •ë„ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤."
TZ = gettz("Asia/Seoul")
TODAY = datetime.now(TZ).date()

# í°íŠ¸ ì„¤ì • ì‹œë„: /fonts/Pretendard-Bold.ttf ì¡´ì¬ ì‹œ ì‚¬ìš© (ì—†ìœ¼ë©´ ìë™ ìƒëµ)
FONT_PATH = "/fonts/Pretendard-Bold.ttf"
CUSTOM_FONT_AVAILABLE = os.path.exists(FONT_PATH)

# API í‚¤ (í™˜ê²½ë³€ìˆ˜ë¡œ ì „ë‹¬ ê¶Œì¥)
KOSIS_API_KEY = os.environ.get("KOSIS_API_KEY", "")
DATA_GO_KEY = os.environ.get("DATA_GO_KR_KEY", "")
SGIS_ACCESS_KEY = os.environ.get("SGIS_ACCESS_KEY", "")
SGIS_SECRET_KEY = os.environ.get("SGIS_SECRET_KEY", "")

# ---------------------------
# ìœ í‹¸ë¦¬í‹°
# ---------------------------
def seoul_midnight_today() -> datetime:
    now = datetime.now(TZ)
    return datetime(now.year, now.month, now.day, 0, 0, 0, tzinfo=TZ)

def remove_future_rows(df: pd.DataFrame, date_col: str = "date") -> pd.DataFrame:
    """ë¡œì»¬ ìì •(Asia/Seoul) ì´í›„ì˜ ë¯¸ë˜ ë°ì´í„° ì œê±°.
    pandas ë²„ì „ì— ë”°ë¼ tz_localizeê°€ errors/nonexistent/ambiguous ì¸ìë¥¼ ì§€ì›í•˜ì§€ ì•ŠëŠ” ê²½ìš°ê°€ ìˆì–´
    ì•ˆì „í•˜ê²Œ ì²˜ë¦¬í•˜ë„ë¡ ìˆ˜ì •.
    """
    if date_col not in df.columns:
        return df
    df = df.copy()
    s = pd.to_datetime(df[date_col], errors="coerce")
    # tz-aware ì—¬ë¶€ í™•ì¸ í›„ ì²˜ë¦¬
    if getattr(s.dt, "tz", None) is None:
        # naive â†’ Asia/Seoul ë¡œì»¬ë¼ì´ì¦ˆ
        s = s.dt.tz_localize(TZ)
    else:
        # ë‹¤ë¥¸ TZë¼ë©´ Asia/Seoulë¡œ ë³€í™˜
        s = s.dt.tz_convert(TZ)
    df[date_col] = s
    cutoff = seoul_midnight_today()
    return df[df[date_col] < cutoff]

def moving_average(series: pd.Series, window: int) -> pd.Series:
    if window is None or window <= 1:
        return series
    return series.rolling(window=window, min_periods=max(1, window//2)).mean()

def try_parse_float(x):
    try:
        return float(x)
    except Exception:
        return np.nan

def color_scale_legend_html(title: str, colors: List[str], ticks: List[str]) -> str:
    # ê°„ë‹¨í•œ HTML ë²”ë¡€ (pydeckê³¼ í•¨ê»˜ í‘œì‹œ)
    rects = "".join([f'<div style="flex:1;height:10px;background:{c};"></div>' for c in colors])
    labels = " | ".join(ticks)
    return f"""
    <div style="font-size:12px;">
      <b>{title}</b>
      <div style="display:flex;gap:0;margin-top:6px;border:1px solid #ddd">{rects}</div>
      <div style="display:flex;justify-content:space-between;font-size:11px;color:#333">{labels}</div>
    </div>
    """

# ---------------------------
# ë°ì´í„° ë¡œë”© (ìºì‹œ)
# ---------------------------

@st.cache_data(show_spinner=False, ttl=60*30)
def fetch_seoul_geojson() -> Tuple[dict, str]:
    """
    ì„œìš¸ì‹œ ìì¹˜êµ¬ ê²½ê³„(GeoJSON)
    1ìˆœìœ„: SGIS/API (ì¸ì¦ í•„ìš”) -> êµ¬í˜„ ìë¦¬(í‚¤ í•„ìš”, ì„œë¹„ìŠ¤ì— ë”°ë¼ ê²½ë¡œ ìƒì´)
    2ìˆœìœ„: data.go.krì˜ í–‰ì •ê²½ê³„(ë‹¤ìš´ë¡œë“œí˜•) â†’ ì§ì ‘ URL ì œê³µ ì‹œ íŒŒì‹±
    3ìˆœìœ„: ê³µê°œ ë°±ì—…(ë¹„ê³µì‹, ë‹¨ìˆœí™” GeoJSON) Fallback

    ë°˜í™˜: (geojson_dict, source_label)
    """
    # --- 1) SGIS ê³µì‹ API (ìƒ˜í”Œ êµ¬ì¡°) ---
    # ë¬¸ì„œ: https://sgis.kostat.go.kr (ì¸ì¦ í›„ ì‚¬ìš©)
    # if SGIS_ACCESS_KEY and SGIS_SECRET_KEY:
    #     try:
    #         # ì‹¤ì œ êµ¬í˜„ ì‹œ: í† í° ë°œê¸‰ â†’ ê²½ê³„ API í˜¸ì¶œ â†’ ì„œìš¸ì‹œ ìì¹˜êµ¬ ë ˆë²¨(ì‹œêµ°êµ¬) GeoJSON ì·¨ë“
    #         # ì•„ë˜ëŠ” êµ¬ì¡° ì˜ˆì‹œ(ë™ì‘ X)
    #         token = "<get_token_with_sgis>"
    #         url = "<sgis-geojson-endpoint-for-seoul-districts>"
    #         r = requests.get(url, headers={"Authorization": f"Bearer {token}"}, timeout=20)
    #         r.raise_for_status()
    #         gj = r.json()
    #         return gj, "SGIS(ê³µì‹)"
    #     except Exception:
    #         pass

    # --- 2) data.go.kr ê³µì‹ íŒŒì¼ (ì‚¬ìš©ìê°€ URL ì£¼ì…/ì„¤ì • ì‹œ) ---
    data_go_kr_geojson_url = os.environ.get("SEOUL_GEOJSON_URL", "")
    if data_go_kr_geojson_url:
        try:
            r = requests.get(data_go_kr_geojson_url, timeout=20)
            r.raise_for_status()
            gj = r.json()
            return gj, "data.go.kr(ê³µì‹)"
        except Exception:
            pass

    # --- 3) Fallback: ê³µê°œ ì €ì¥ì†Œ(ë¹„ê³µì‹) ë‹¨ìˆœí™” GeoJSON ---
    try:
        url = "https://raw.githubusercontent.com/southkorea/seoul-maps/master/json/seoul_municipalities_geo_simple.json"
        r = requests.get(url, timeout=20)
        r.raise_for_status()
        return r.json(), "ëŒ€ì²´(ë¹„ê³µì‹) GitHub"
    except Exception:
        # ë§ˆì§€ë§‰ ë°©ì–´ì„ : ë§¤ìš° ë‹¨ìˆœí•œ placeholder GeoJSON
        placeholder = {
            "type": "FeatureCollection",
            "features": []
        }
        return placeholder, "ë‚´ì¥ ì˜ˆì‹œ(ê²½ê³„ ì—†ìŒ)"

@st.cache_data(show_spinner=False, ttl=60*30)
def fetch_closed_schools() -> Tuple[pd.DataFrame, str, str]:
    """
    ì„œìš¸ì‹œ íêµ í˜„í™© (êµ¬ë³„)
    1) data.go.kr 'íêµ í˜„í™©' ë°ì´í„°ì…‹ API/íŒŒì¼ ì‹œë„ (í‚¤ í•„ìš”)
    2) KEIS/êµìœ¡ë¶€ ê³µê°œìë£Œ ì‹œë„
    3) Fallback: ì˜ˆì‹œ ë°ì´í„° ìƒì„±(í•™êµëª…/íêµì—°ë„/ì¢Œí‘œ ì¼ë¶€/êµ¬ëª…)
    í‘œì¤€ ìŠ¤í‚¤ë§ˆ: ['date','value','group']ëŠ” ì§‘ê³„í‘œì— ì ìš©, ì›ìë£ŒëŠ” ë³„ë„ ì»¬ëŸ¼ ìœ ì§€

    ë°˜í™˜: (ì›ìë£Œ df, source_label, collected_date_str)
    """
    collected_date = TODAY.strftime("%Y-%m-%d")
    endpoint = os.environ.get("DATA_GO_CLOSED_SCHOOL_URL", "")
    try:
        if endpoint:
            if endpoint.lower().endswith(".csv"):
                df = pd.read_csv(endpoint)
            else:
                r = requests.get(endpoint, params={"serviceKey": DATA_GO_KEY}, timeout=20)
                r.raise_for_status()
                js = r.json()
                rows = js.get("data", js.get("items", []))
                df = pd.json_normalize(rows)
            possible_sido_cols = [c for c in df.columns if "ì‹œë„" in c or "ê´‘ì—­" in c or "ì‹œë„ëª…" in c]
            if possible_sido_cols:
                col = possible_sido_cols[0]
                df = df[df[col].astype(str).str.contains("ì„œìš¸")]
            return df.reset_index(drop=True), "data.go.kr(ê³µì‹)", collected_date
    except Exception:
        pass

    keis_url = os.environ.get("KEIS_CLOSED_SCHOOL_URL", "")
    try:
        if keis_url:
            if keis_url.lower().endswith(".csv"):
                df = pd.read_csv(keis_url)
            else:
                r = requests.get(keis_url, timeout=20)
                r.raise_for_status()
                df = pd.read_csv(io.StringIO(r.text))
            possible_sido_cols = [c for c in df.columns if "ì‹œë„" in c or "ê´‘ì—­" in c or "ì‹œë„ëª…" in c]
            if possible_sido_cols:
                col = possible_sido_cols[0]
                df = df[df[col].astype(str).str.contains("ì„œìš¸")]
            return df.reset_index(drop=True), "KEIS/êµìœ¡ë¶€(ê³µì‹)", collected_date
    except Exception:
        pass

    example = [
        ["êµ¬ì˜ì´ˆë“±í•™êµ(ì˜ˆì‹œ)", 2002, 37.537, 127.091, "ê´‘ì§„êµ¬"],
        ["ì„œì´ˆì¤‘í•™êµ(ì˜ˆì‹œ)",   2005, 37.476, 127.014, "ì„œì´ˆêµ¬"],
        ["ê³ ì²™ê³ ë“±í•™êµ(ì˜ˆì‹œ)", 2011, 37.506, 126.861, "êµ¬ë¡œêµ¬"],
        ["í™ì œì´ˆ(ì˜ˆì‹œ)",       2008, 37.590, 126.945, "ì„œëŒ€ë¬¸êµ¬"],
        ["í•œê°•ì´ˆ(ì˜ˆì‹œ)",       2015, 37.528, 126.932, "ì˜ë“±í¬êµ¬"],
        ["ì ì‹ ì´ˆ(ì˜ˆì‹œ)",       2016, 37.513, 127.095, "ì†¡íŒŒêµ¬"],
    ]
    df = pd.DataFrame(example, columns=["í•™êµëª…","íêµì—°ë„","ìœ„ë„","ê²½ë„","ìì¹˜êµ¬"])
    return df, "ë‚´ì¥ ì˜ˆì‹œ", collected_date

@st.cache_data(show_spinner=False, ttl=60*30)
def fetch_population_components() -> Tuple[pd.DataFrame, str, str]:
    """
    ì¸êµ¬ì†Œë©¸ì§€í‘œ êµ¬ì„±ìš”ì†Œ:
      - 20â€“39ì„¸ ì—¬ì„± ì¸êµ¬ (ë¶„ì ë˜ëŠ” ë¶„ëª¨)
      - 65ì„¸ ì´ìƒ ì¸êµ¬
    KOSTAT/KOSIS APIë¥¼ ìš°ì„  ì‹œë„ â†’ ì‹¤íŒ¨ ì‹œ data.go.kr ì¸êµ¬ íŒŒì¼ â†’ ìµœì¢… ì˜ˆì‹œ ìƒì„±.
    í‘œì¤€ ìŠ¤í‚¤ë§ˆ(ì§‘ê³„í˜•): ['date','value','group'], group=ìì¹˜êµ¬

    ë°˜í™˜: (wide ë˜ëŠ” longí˜• ì›ìë£Œ df, source_label, collected_date_str)
    """
    collected_date = TODAY.strftime("%Y-%m-%d")

    if KOSIS_API_KEY:
        try:
            pass
        except Exception:
            pass

    pop_url = os.environ.get("DATA_GO_POP_URL", "")
    if pop_url:
        try:
            if pop_url.lower().endswith(".csv"):
                df = pd.read_csv(pop_url)
            else:
                r = requests.get(pop_url, params={"serviceKey": DATA_GO_KEY}, timeout=30)
                r.raise_for_status()
                js = r.json()
                rows = js.get("data", js.get("items", []))
                df = pd.json_normalize(rows)
            return df, "data.go.kr(ê³µì‹)", collected_date
        except Exception:
            pass

    rng = np.random.default_rng(42)
    gus = [
        "ì¢…ë¡œêµ¬","ì¤‘êµ¬","ìš©ì‚°êµ¬","ì„±ë™êµ¬","ê´‘ì§„êµ¬","ë™ëŒ€ë¬¸êµ¬","ì¤‘ë‘êµ¬","ì„±ë¶êµ¬","ê°•ë¶êµ¬","ë„ë´‰êµ¬",
        "ë…¸ì›êµ¬","ì€í‰êµ¬","ì„œëŒ€ë¬¸êµ¬","ë§ˆí¬êµ¬","ì–‘ì²œêµ¬","ê°•ì„œêµ¬","êµ¬ë¡œêµ¬","ê¸ˆì²œêµ¬","ì˜ë“±í¬êµ¬","ë™ì‘êµ¬",
        "ê´€ì•…êµ¬","ì„œì´ˆêµ¬","ê°•ë‚¨êµ¬","ì†¡íŒŒêµ¬","ê°•ë™êµ¬"
    ]
    years = list(range(2010, TODAY.year+1))
    records = []
    for y in years:
        for g in gus:
            female_20_39 = rng.integers(8000, 45000)
            aged_65_plus = rng.integers(6000, 80000)
            total = rng.integers(150000, 600000)
            records.append([y, g, female_20_39, aged_65_plus, total])
    df = pd.DataFrame(records, columns=["ì—°ë„","ìì¹˜êµ¬","ì—¬ì„±20_39","ê³ ë ¹65_ì´ìƒ","ì´ì¸êµ¬"])
    return df, "ë‚´ì¥ ì˜ˆì‹œ", collected_date

# ---------------------------
# ë„í˜• ìœ í‹¸ (êµ¬ ì¤‘ì‹¬ ì¶”ì •)
# ---------------------------
def feature_centroid(feature: dict) -> Tuple[float, float] | None:
    """
    GeoJSON Featureì˜ ì¤‘ì‹¬ì¢Œí‘œ(ê²½ë„, ìœ„ë„) ì¶”ì • (geopandas ì—†ì´)
    Polygon/MultiPolygonì˜ ëª¨ë“  ì¢Œí‘œ í‰ê· ì„ ì‚¬ìš©(ë©´ì  ê°€ì¤‘ì¹˜ ì—†ìŒ â†’ ê·¼ì‚¬ì¹˜).
    """
    try:
        geom = feature.get("geometry", {})
        gtype = geom.get("type", "")
        coords = geom.get("coordinates", [])
        xs, ys, n = 0.0, 0.0, 0
        def add_coords(crds):
            nonlocal xs, ys, n
            for (x, y) in crds:
                xs += float(x); ys += float(y); n += 1

        if gtype == "Polygon":
            for ring in coords:
                add_coords(ring)
        elif gtype == "MultiPolygon":
            for poly in coords:
                for ring in poly:
                    add_coords(ring)
        if n == 0:
            return None
        return xs/n, ys/n
    except Exception:
        return None

def build_gu_centroids(geojson: dict, gu_key_candidates: List[str]) -> Dict[str, Tuple[float,float]]:
    name_map = {}
    for feat in geojson.get("features", []):
        props = feat.get("properties", {})
        gu_name = None
        for key in gu_key_candidates:
            if key in props:
                gu_name = str(props[key])
                break
        if not gu_name:
            if "name" in props:
                gu_name = str(props["name"])
        if not gu_name:
            continue
        c = feature_centroid(feat)
        if c:
            name_map[gu_name] = (c[1], c[0])  # (lat, lon)
    return name_map

# ---------------------------
# ì „ì²˜ë¦¬ / íŒŒìƒ: ì¸êµ¬ì†Œë©¸ì§€í‘œ
# ---------------------------
def compute_extinction_index(df_raw: pd.DataFrame) -> pd.DataFrame:
    """
    ê°„ë‹¨ ì •ì˜(ëª…ì‹œ): ì¸êµ¬ì†Œë©¸ì§€ìˆ˜ ì˜ˆì‹œ
      - index = (ì—¬ì„± 20â€“39ì„¸ ì¸êµ¬) / (65ì„¸ ì´ìƒ ì¸êµ¬) * 100
    ì—°êµ¬Â·ê¸°ê´€ë³„ ì •ì˜ëŠ” ë‹¤ì–‘í•¨. ë³¸ ì•±ì—ì„œëŠ” ìœ„ ì •ì˜ ì‚¬ìš©ì„ *í™”ë©´ì— ëª…ì‹œ*.
    í‘œì¤€ ìŠ¤í‚¤ë§ˆ(long):
      date(ì—°-01-01), value(ì§€ìˆ˜ ë˜ëŠ” ë¹„ìœ¨), group(ìì¹˜êµ¬), metric("ext_index" ë“±)
    """
    df = df_raw.copy()
    col_y = next((c for c in df.columns if "ì—°ë„" in c or c.lower()=="year"), None)
    col_gu = next((c for c in df.columns if "ìì¹˜êµ¬" in c or "êµ¬"==c or "district" in c.lower()), None)
    col_f2039 = next((c for c in df.columns if "ì—¬ì„±20" in c or "20_39" in c or ("ì—¬ì„±" in c and "20" in c)), None)
    col_65 = next((c for c in df.columns if ("65" in c and "ì´ìƒ" in c) or "aged" in c.lower()), None)

    if not (col_y and col_gu and col_f2039 and col_65):
        col_y, col_gu, col_f2039, col_65 = "ì—°ë„","ìì¹˜êµ¬","ì—¬ì„±20_39","ê³ ë ¹65_ì´ìƒ"

    df = df[[col_y, col_gu, col_f2039, col_65]].rename(columns={
        col_y:"ì—°ë„", col_gu:"ìì¹˜êµ¬", col_f2039:"ì—¬ì„±20_39", col_65:"ê³ ë ¹65_ì´ìƒ"
    })
    for c in ["ì—¬ì„±20_39","ê³ ë ¹65_ì´ìƒ"]:
        df[c] = pd.to_numeric(df[c], errors="coerce")

    df["ì§€ìˆ˜"] = (df["ì—¬ì„±20_39"] / df["ê³ ë ¹65_ì´ìƒ"]).replace([np.inf, -np.inf], np.nan) * 100.0
    df["date"] = pd.to_datetime(df["ì—°ë„"].astype(int).astype(str) + "-01-01")
    # tz-naive â†’ Asia/Seoul ë¡œì»¬ë¼ì´ì¦ˆ
    if getattr(df["date"].dt, "tz", None) is None:
        df["date"] = df["date"].dt.tz_localize(TZ)
    else:
        df["date"] = df["date"].dt.tz_convert(TZ)

    out = df[["date","ì§€ìˆ˜","ìì¹˜êµ¬"]].rename(columns={"ì§€ìˆ˜":"value","ìì¹˜êµ¬":"group"})
    out = out.dropna().sort_values(["group","date"]).reset_index(drop=True)
    out = remove_future_rows(out, "date")
    out["metric"] = "ext_index"
    return out

# ---------------------------
# ì‹œê°í™”
# ---------------------------
def choropleth_extinction(df_long: pd.DataFrame, gj: dict, gu_name_keys: List[str], year: int, unit: str):
    """ì¸êµ¬ì†Œë©¸ Choropleth (ì—°ë„ ë‹¨ë©´)"""
    df_y = df_long[df_long["date"].dt.year == year]
    mapper = {r["group"]: r["value"] for _, r in df_y.iterrows()}

    min_v = df_y["value"].min() if not df_y.empty else 0
    max_v = df_y["value"].max() if not df_y.empty else 1
    def norm(v): 
        if math.isfinite(min_v) and math.isfinite(max_v) and max_v != min_v:
            return (v - min_v) / (max_v - min_v)
        return 0.5

    def ramp(t):
        r = int(255 * t)
        g = int(120 * (1-t) + 30)
        b = int(255 * (1-t))
        return [r,g,b,160]

    layers = []
    for feat in gj.get("features", []):
        props = feat.get("properties", {})
        gu = None
        for k in gu_name_keys:
            if k in props:
                gu = str(props[k]); break
        if not gu and "name" in props:
            gu = str(props["name"])
        val = mapper.get(gu, np.nan)
        t = norm(val) if pd.notna(val) else 0.0
        color = ramp(t)
        layers.append({
            "type":"PolygonLayer",
            "data":[feat],
            "get_polygon":"function f(d){return d.geometry.coordinates}",
            "get_fill_color": color,
            "stroked": True,
            "get_line_color":[80,80,80,200],
            "line_width_min_pixels": 1,
            "pickable": True,
            "autoHighlight": True,
            "tooltip": True
        })

    deck_layers = []
    for L in layers:
        deck_layers.append(
            pdk.Layer(
                "PolygonLayer",
                L["data"],
                get_polygon="geometry.coordinates",
                get_fill_color=L["get_fill_color"],
                stroked=True,
                get_line_color=L["get_line_color"],
                line_width_min_pixels=L["line_width_min_pixels"],
                pickable=True,
                auto_highlight=True,
            )
        )

    view_state = pdk.ViewState(latitude=37.5665, longitude=126.9780, zoom=9.5, pitch=0)
    r = pdk.Deck(
        layers=deck_layers,
        initial_view_state=view_state,
        tooltip={"html":"<b>{name}</b>", "style":{"color":"white"}},
        map_style="light"
    )
    st.pydeck_chart(r, use_container_width=True)

    legend_html = color_scale_legend_html(
        "ì¸êµ¬ì†Œë©¸ì§€ìˆ˜(ì—¬ì„±20â€“39ì„¸/65ì„¸ì´ìƒ Ã—100)",
        ["#0000FF","#7F60B0","#BF6090","#FF0000"],
        [f"{min_v:.1f}", f"{(min_v+max_v)/2:.1f}", f"{max_v:.1f}"]
    )
    st.markdown(legend_html, unsafe_allow_html=True)

def points_closed_schools(df_points: pd.DataFrame, gu_centroids: Dict[str,Tuple[float,float]]):
    """
    íêµ ì  ë ˆì´ì–´: ìœ„ê²½ë„ ì—†ìœ¼ë©´ êµ¬ ì¤‘ì‹¬ìœ¼ë¡œ ëŒ€ì²´
    ì…ë ¥ df: í•™êµëª…, íêµì—°ë„, ìœ„ë„, ê²½ë„, ìì¹˜êµ¬
    """
    df = df_points.copy()
    if "ìœ„ë„" not in df.columns or "ê²½ë„" not in df.columns:
        df["lat"] = df["ìì¹˜êµ¬"].map(lambda g: gu_centroids.get(g, (np.nan,np.nan))[0])
        df["lon"] = df["ìì¹˜êµ¬"].map(lambda g: gu_centroids.get(g, (np.nan,np.nan))[1])
    else:
        df["lat"] = pd.to_numeric(df["ìœ„ë„"], errors="coerce")
        df["lon"] = pd.to_numeric(df["ê²½ë„"], errors="coerce")

    df = df.dropna(subset=["lat","lon"])
    layer = pdk.Layer(
        "ScatterplotLayer",
        df,
        get_position='[lon, lat]',
        get_radius=80,
        get_fill_color=[200, 30, 0, 180],
        pickable=True,
    )
    view_state = pdk.ViewState(latitude=37.5665, longitude=126.9780, zoom=10.2, pitch=0)
    deck = pdk.Deck(layers=[layer], initial_view_state=view_state, map_style="light",
                    tooltip={"html":"<b>{í•™êµëª…}</b><br/>íêµì—°ë„: {íêµì—°ë„}<br/>{ìì¹˜êµ¬}"})
    st.pydeck_chart(deck, use_container_width=True)

# ---------------------------
# ì•± UI
# ---------------------------
st.set_page_config(page_title=APP_TITLE, layout="wide")

# ìƒë‹¨ ì œëª©/ì„¤ëª… + ìµœì‹ í™” ì•ˆë‚´/ë¯¸ë˜ë°ì´í„° ì œê±° ë°°ë„ˆ
st.title(APP_TITLE)
st.write(APP_DESC)
st.info("ğŸ”„ ë°ì´í„°ëŠ” ê³µì‹ ê³µê°œ ë°ì´í„°ë¥¼ ìš°ì„  ì‚¬ìš©í•˜ë©°, ì—°ê²° ì‹¤íŒ¨ ì‹œ ëŒ€ì²´ ì¶œì²˜ ë˜ëŠ” ì˜ˆì‹œ ë°ì´í„°ë¡œ ìë™ ì „í™˜í•©ë‹ˆë‹¤. "
        "ëª¨ë“  ì‹œê³„ì—´ì€ ë¡œì»¬ ìì •(Asia/Seoul) ì´í›„ì˜ ë¯¸ë˜ ë°ì´í„°ê°€ ìë™ ì œê±°ë©ë‹ˆë‹¤.")

# ë°ì´í„° ë¡œë”©
geojson, geo_source = fetch_seoul_geojson()
closed_df_raw, closed_source, closed_collected = fetch_closed_schools()
pop_df_raw, pop_source, pop_collected = fetch_population_components()

# ë°ì´í„° ì†ŒìŠ¤ ë°°ë„ˆ
if "ì˜ˆì‹œ" in (geo_source + closed_source + pop_source):
    st.warning("âš ï¸ ì¼ë¶€ ë°ì´í„°ëŠ” ì˜ˆì‹œ/ëŒ€ì²´ ì¶œì²˜ë¥¼ ì‚¬ìš© ì¤‘ì…ë‹ˆë‹¤. ì‹¤ì œ ë¶„ì„ ì „ ê³µì‹ ë°ì´í„° ì—°ê²°/í‚¤ ì„¤ì •ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")

# ì‚¬ì´ë“œë°” ì»¨íŠ¸ë¡¤
st.sidebar.header("í•„í„°")
year_min = 2000
year_max = TODAY.year
sel_year = st.sidebar.slider("ì—°ë„ ì„ íƒ", min_value=year_min, max_value=year_max, value=min(year_max, max(year_min, TODAY.year-1)))
metric_choice = st.sidebar.radio("ì§€í‘œ ì„ íƒ", ["íêµ í˜„í™©", "ì¸êµ¬ì†Œë©¸ ì§€í‘œ"], index=0)
smooth_win = st.sidebar.select_slider("ìŠ¤ë¬´ë”©(ì´ë™í‰ê·  ìœˆë„ìš°)", options=[1,3,5], value=1)
unit_choice = st.sidebar.radio("ë‹¨ìœ„", ["ì§€ìˆ˜(Ã—100)", "ë¹„ìœ¨"], index=0, help="ë³¸ ì•±ì—ì„œëŠ” ì§€ìˆ˜=ì—¬ì„±20â€“39ì„¸/65ì„¸ì´ìƒÃ—100")

# êµ¬ ì„ íƒ
all_gus = [
    "ì¢…ë¡œêµ¬","ì¤‘êµ¬","ìš©ì‚°êµ¬","ì„±ë™êµ¬","ê´‘ì§„êµ¬","ë™ëŒ€ë¬¸êµ¬","ì¤‘ë‘êµ¬","ì„±ë¶êµ¬","ê°•ë¶êµ¬","ë„ë´‰êµ¬",
    "ë…¸ì›êµ¬","ì€í‰êµ¬","ì„œëŒ€ë¬¸êµ¬","ë§ˆí¬êµ¬","ì–‘ì²œêµ¬","ê°•ì„œêµ¬","êµ¬ë¡œêµ¬","ê¸ˆì²œêµ¬","ì˜ë“±í¬êµ¬","ë™ì‘êµ¬",
    "ê´€ì•…êµ¬","ì„œì´ˆêµ¬","ê°•ë‚¨êµ¬","ì†¡íŒŒêµ¬","ê°•ë™êµ¬"
]
sel_gus = st.sidebar.multiselect("êµ¬ ì„ íƒ(ì‹œê³„ì—´ ë¹„êµ)", options=all_gus, default=["ì¢…ë¡œêµ¬","ì„œì´ˆêµ¬","ê°•ë‚¨êµ¬"])

# ---------------------------
# ì „ì²˜ë¦¬: íêµ ì§‘ê³„ í…Œì´ë¸” (í‘œì¤€ ìŠ¤í‚¤ë§ˆ)
# ---------------------------
def build_closed_agg(df_raw: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:
    df = df_raw.copy()
    if "ìì¹˜êµ¬" not in df.columns:
        gu_col = next((c for c in df.columns if "êµ¬" in c), None)
        if gu_col:
            df = df.rename(columns={gu_col:"ìì¹˜êµ¬"})
    if "íêµì—°ë„" not in df.columns:
        ycol = next((c for c in df.columns if "ì—°ë„" in c or "ë…„ë„" in c), None)
        if ycol:
            df = df.rename(columns={ycol:"íêµì—°ë„"})

    if "íêµì—°ë„" in df.columns:
        df["íêµì—°ë„"] = pd.to_numeric(df["íêµì—°ë„"], errors="coerce").astype("Int64")
    agg = df.dropna(subset=["ìì¹˜êµ¬","íêµì—°ë„"]).groupby(["ìì¹˜êµ¬","íêµì—°ë„"]).size().reset_index(name="íêµìˆ˜")
    agg["date"] = pd.to_datetime(agg["íêµì—°ë„"].astype(int).astype(str) + "-01-01")
    if getattr(agg["date"].dt, "tz", None) is None:
        agg["date"] = agg["date"].dt.tz_localize(TZ)
    else:
        agg["date"] = agg["date"].dt.tz_convert(TZ)
    agg = agg.rename(columns={"íêµìˆ˜":"value","ìì¹˜êµ¬":"group"})
    agg = remove_future_rows(agg, "date").sort_values(["group","date"]).reset_index(drop=True)
    return df, agg[["date","value","group"]]

# ---------------------------
# ì „ì²˜ë¦¬: ì¸êµ¬ì†Œë©¸ì§€ìˆ˜ (í‘œì¤€ ìŠ¤í‚¤ë§ˆ)
# ---------------------------
ext_long = compute_extinction_index(pop_df_raw)
if unit_choice == "ë¹„ìœ¨":
    ext_long = ext_long.copy()
    ext_long["value"] = ext_long["value"] / 100.0

if smooth_win and smooth_win > 1:
    ext_long = (
        ext_long.sort_values(["group","date"])
        .groupby("group", group_keys=False)
        .apply(lambda d: d.assign(value=moving_average(d["value"], smooth_win)))
        .reset_index(drop=True)
    )

# ---------------------------
# ì§€ë„ìš© ì¤‘ì‹¬ ì¢Œí‘œ ì‚¬ì „
# ---------------------------
gu_name_keys = ["name_2", "SIG_KOR_NM", "SIG_ENG_NM", "adm_nm", "EMD_KOR_NM", "name"]
gu_centroids = build_gu_centroids(geojson, gu_name_keys)

# ---------------------------
# íƒ­ êµ¬ì„±
# ---------------------------
tab1, tab2, tab3 = st.tabs(["ğŸ—ºï¸ íêµ ì§€ë„", "ğŸ—ºï¸ ì¸êµ¬ì†Œë©¸ ì§€ë„", "ğŸ“„ í‘œ/ë‹¤ìš´ë¡œë“œ"])

with tab1:
    st.subheader("ì„œìš¸ì‹œ íêµ í˜„í™© (ìì¹˜êµ¬)")
    raw_closed, closed_long = build_closed_agg(closed_df_raw)

    try:
        points_closed_schools(raw_closed, gu_centroids)
    except Exception as e:
        st.error(f"íêµ ì  ë ˆì´ì–´ í‘œì‹œ ì¤‘ ì˜¤ë¥˜: {e}")

    st.markdown("**ì„¸ë¶€ ëª©ë¡ (ì„ íƒ êµ¬ í•„í„° ì ìš©)**")
    show_df = raw_closed.copy()
    if sel_gus:
        show_df = show_df[show_df["ìì¹˜êµ¬"].isin(sel_gus)]
    st.dataframe(show_df.reset_index(drop=True), use_container_width=True)

    st.markdown(f"**{sel_year}ë…„ êµ¬ë³„ íêµ ê±´ìˆ˜ (í‘œì¤€ ìŠ¤í‚¤ë§ˆ)**")
    agg_year = (
        closed_long[closed_long["date"].dt.year == sel_year]
        .groupby("group", as_index=False)["value"].sum()
        .rename(columns={"group":"ìì¹˜êµ¬","value":"íêµìˆ˜"})
        .sort_values("íêµìˆ˜", ascending=False)
    )
    st.dataframe(agg_year, use_container_width=True)

with tab2:
    st.subheader("ì„œìš¸ì‹œ ì¸êµ¬ì†Œë©¸ ì§€í‘œ (ìì¹˜êµ¬)")
    st.caption("ì •ì˜: ì¸êµ¬ì†Œë©¸ì§€ìˆ˜ = (ì—¬ì„± 20â€“39ì„¸ ì¸êµ¬ / 65ì„¸ ì´ìƒ ì¸êµ¬) Ã— 100  (ë³¸ ì•±ì˜ ë‹¨ìˆœ ì •ì˜)")
    try:
        choropleth_extinction(ext_long, geojson, gu_name_keys, sel_year, unit_choice)
    except Exception as e:
        st.error(f"ì¸êµ¬ì†Œë©¸ Choropleth í‘œì‹œ ì¤‘ ì˜¤ë¥˜: {e}")

    st.markdown("**ì„ íƒ êµ¬ ì‹œê³„ì—´ ë¹„êµ (êº¾ì€ì„ )**")
    line_df = ext_long[ext_long["group"].isin(sel_gus)].copy()
    if line_df.empty:
        st.info("ì„ íƒí•œ êµ¬ì˜ ì‹œê³„ì—´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        fig = px.line(
            line_df.assign(ì—°ë„=line_df["date"].dt.year),
            x="ì—°ë„", y="value", color="group",
            labels={"value":"ì§€í‘œ ê°’","group":"ìì¹˜êµ¬"},
            title=None
        )
        if CUSTOM_FONT_AVAILABLE:
            fig.update_layout(font_family="Pretendard")
        st.plotly_chart(fig, use_container_width=True)

with tab3:
    st.subheader("í‘œì¤€ ìŠ¤í‚¤ë§ˆ í…Œì´ë¸” & CSV ë‹¤ìš´ë¡œë“œ")
    raw_closed, closed_long = build_closed_agg(closed_df_raw)

    st.markdown("**íêµ(êµ¬ë³„Â·ì—°ë„ë³„ ì§‘ê³„): columns = [date, value, group]**")
    st.dataframe(closed_long, use_container_width=True)
    csv_closed = closed_long.to_csv(index=False).encode("utf-8-sig")
    st.download_button("íêµ ì§‘ê³„ CSV ë‹¤ìš´ë¡œë“œ", csv_closed, file_name="closed_schools_agg.csv", mime="text/csv")

    st.markdown("**ì¸êµ¬ì†Œë©¸ ì§€í‘œ(êµ¬ë³„Â·ì—°ë„ë³„): columns = [date, value, group, metric]**")
    st.dataframe(ext_long, use_container_width=True)
    csv_ext = ext_long.to_csv(index=False).encode("utf-8-sig")
    st.download_button("ì¸êµ¬ì†Œë©¸ ì§€í‘œ CSV ë‹¤ìš´ë¡œë“œ", csv_ext, file_name="extinction_index.csv", mime="text/csv")

# ---------------------------
# ì¶œì²˜/ìˆ˜ì§‘ì¼ì/ë¼ì´ì„ ìŠ¤/í°íŠ¸ ì•ˆë‚´
# ---------------------------
st.divider()
st.markdown("### ğŸ”— ì¶œì²˜(ìš°ì„ ìˆœìœ„), ìˆ˜ì§‘ì¼ì, ë¼ì´ì„ ìŠ¤ ê³ ì§€")
st.markdown(f"""
- **í–‰ì •ê²½ê³„(ì„œìš¸ì‹œ ìì¹˜êµ¬)**: {geo_source}  
  - SGIS(í†µê³„ì§€ë¦¬ì •ë³´ì„œë¹„ìŠ¤) API ê¶Œì¥: https://sgis.kostat.go.kr  
  - (ëŒ€ì²´ ì‚¬ìš© ì‹œ) GitHub ë‹¨ìˆœí™” GeoJSON (ë¹„ê³µì‹)  
- **íêµ í˜„í™©**: {closed_source}, ìˆ˜ì§‘ì¼ì: {closed_collected}  
  - ê¶Œì¥: data.go.kr(êµìœ¡ë¶€/KEIS) 'íêµ í˜„í™©' ë°ì´í„°ì…‹ ë˜ëŠ” CSV  
- **ì¸êµ¬ êµ¬ì„±(ì—¬ì„± 20â€“39, 65+)**: {pop_source}, ìˆ˜ì§‘ì¼ì: {pop_collected}  
  - ê¶Œì¥: KOSTAT/KOSIS OpenAPI ì‚¬ìš©ì í†µê³„í‘œ
- **ë¼ì´ì„ ìŠ¤**: ê° ì¶œì²˜ì˜ ì´ìš©ì•½ê´€/ì €ì‘ê¶Œ ì§€ì¹¨ì„ ì¤€ìˆ˜í•˜ì„¸ìš”.
""")

st.markdown("""
**í™˜ê²½ë³€ìˆ˜ë¡œ URL/í‚¤ ì£¼ì… ì˜ˆì‹œ (Codespaces):**
- `SEOUL_GEOJSON_URL`: data.go.kr ë“± ê³µì‹ GeoJSON íŒŒì¼ URL
- `DATA_GO_CLOSED_SCHOOL_URL`: íêµ í˜„í™© CSV/JSON URL
- `DATA_GO_POP_URL`: ì¸êµ¬ êµ¬ì„± CSV/JSON URL
- `KOSIS_API_KEY`, `DATA_GO_KR_KEY`, `SGIS_ACCESS_KEY`, `SGIS_SECRET_KEY`
""")

if CUSTOM_FONT_AVAILABLE:
    st.caption("í°íŠ¸: /fonts/Pretendard-Bold.ttf ì ìš© ì‹œë„ ì™„ë£Œ(ê°€ëŠ¥ ë²”ìœ„).")
else:
    st.caption("í°íŠ¸: /fonts/Pretendard-Bold.ttf ë¯¸ì¡´ì¬ë¡œ ê¸°ë³¸ í°íŠ¸ ì‚¬ìš© ì¤‘.")
